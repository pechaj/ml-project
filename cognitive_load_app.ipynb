{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0390d273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ad498",
   "metadata": {},
   "source": [
    "## Cognitive Load Classification from Physiological Signals\n",
    "# This notebook demonstrates processing and classification of cognitive load using physiological signals (ECG and EDA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d5fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install neurokit2 matplotlib numpy pandas scikit-learn torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53b480ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score, balanced_accuracy_score\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502bc02d",
   "metadata": {},
   "source": [
    "## Define Dataset and Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896837d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class CognitiveLoadDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "def load_data_from_folders(base_dir, balance_classes=True):\n",
    "    \"\"\"\n",
    "    Loads ECG and EDA data from specified directory structure.\n",
    "    Implements window segmentation with 50% overlap.\n",
    "\n",
    "    Args:\n",
    "        base_dir: Directory containing High_load and Low_load folders\n",
    "        balance_classes: Whether to compute sample weights for class balancing\n",
    "\n",
    "    Returns:\n",
    "        X: Feature array\n",
    "        y: Labels array\n",
    "        sample_weights: Optional weights for balanced sampling\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    window_size = 256 * 20  # ~20 seconds at 256 Hz\n",
    "    step_size = window_size // 2  # 50% overlap\n",
    "\n",
    "    class_samples = {0: [], 1: []}  # For storing samples by class\n",
    "\n",
    "    for label, class_name in enumerate([\"Low_load\", \"High_load\"]):\n",
    "        class_dir = os.path.join(base_dir, class_name)\n",
    "        recordings = {}\n",
    "\n",
    "        for file in os.listdir(class_dir):\n",
    "            parts = file.rsplit('_', 2)\n",
    "            if len(parts) != 3:\n",
    "                continue  # Skip unexpected filenames\n",
    "            subject_id, rec_num, signal_type = parts\n",
    "            subject_key = f\"{subject_id}_{rec_num}\"\n",
    "            signal_type = signal_type.replace('.csv', '').lower()\n",
    "\n",
    "            if subject_key not in recordings:\n",
    "                recordings[subject_key] = {}\n",
    "\n",
    "            file_path = os.path.join(class_dir, file)\n",
    "            recordings[subject_key][signal_type] = pd.read_csv(file_path)\n",
    "\n",
    "        for subject_key, signals in recordings.items():\n",
    "            if 'ecg' in signals: # Only require ECG\n",
    "            # if 'ecg' in signals and 'eda' in signals: # Only require ECG and EDA\n",
    "                ecg = signals['ecg']\n",
    "                min_len = len(ecg)\n",
    "                ecg = ecg[:min_len]\n",
    "                # eda = signals['eda']\n",
    "\n",
    "                # min_len = min(len(ecg), len(eda))\n",
    "                # ecg, eda = ecg[:min_len], eda[:min_len]\n",
    "\n",
    "\n",
    "                for start in range(0, min_len - window_size + 1, step_size):\n",
    "                    ecg_window = ecg[start:start + window_size]\n",
    "                    # Select only ECG signal\n",
    "                    combined_signal = ecg_window\n",
    "                    # eda_window = eda[start:start + window_size]\n",
    "                    # combined_signal = pd.concat([ecg_window, eda_window], axis=1)\n",
    "                    # Store samples by class for balancing\n",
    "                    class_samples[label].append(combined_signal)\n",
    "\n",
    "    # Combine data\n",
    "    for label in [0, 1]:\n",
    "        for sample in class_samples[label]:\n",
    "            X.append(sample)\n",
    "            y.append(label)\n",
    "\n",
    "    # Print original class distribution\n",
    "    class_0_size = y.count(0)\n",
    "    class_1_size = y.count(1)\n",
    "    print(f\"Original class distribution - Class 0: {class_0_size}, Class 1: {class_1_size}\")\n",
    "\n",
    "    # Convert list of DataFrames to a 3D numpy array\n",
    "    X = np.stack([sample.values for sample in X])\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Optionally compute sample weights for balancing classes\n",
    "    if balance_classes:\n",
    "        class_counts = np.bincount(y)\n",
    "        class_weights = 1. / class_counts\n",
    "        sample_weights = class_weights[y]\n",
    "        print(f\"Class weights for balancing: Class 0: {class_weights[0]:.4f}, Class 1: {class_weights[1]:.4f}\")\n",
    "        return X, y, sample_weights\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def create_data_loaders(X, y, sample_weights=None, batch_size=32, test_split=0.2):\n",
    "    \"\"\"\n",
    "    Create balanced data loaders using sample weights\n",
    "    \"\"\"\n",
    "    # Split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "        X, y, np.arange(len(X)), test_size=test_split, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = CognitiveLoadDataset(X_train, y_train)\n",
    "    test_dataset = CognitiveLoadDataset(X_test, y_test)\n",
    "\n",
    "    # Create data loaders\n",
    "    if sample_weights is not None:\n",
    "        # Get sample weights for training samples\n",
    "        train_sample_weights = sample_weights[idx_train]\n",
    "\n",
    "        # Create a sampler\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=train_sample_weights,\n",
    "            num_samples=len(train_sample_weights),\n",
    "            replacement=True\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=train_sampler\n",
    "        )\n",
    "    else:\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def plot_ecg_eda_window(X, y, index=0, sampling_rate=256):\n",
    "    \"\"\"\n",
    "    Plot ECG and EDA signals from a specific window\n",
    "    \"\"\"\n",
    "    sample = X[index]\n",
    "    # window_size = sample.shape[0]\n",
    "    window_size = sample.shape[0] // 2\n",
    "\n",
    "\n",
    "    # Get ECG and EDA segments\n",
    "    ecg = sample[:window_size, 0]\n",
    "    eda = sample[:window_size, 1]\n",
    "\n",
    "\n",
    "    time = np.linspace(0, window_size / sampling_rate, window_size)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(time, ecg, label='ECG', color='red')\n",
    "    plt.title(f'ECG Signal (label: {y[index]})')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(time, eda, label='EDA', color='blue')\n",
    "    plt.title(f'EDA Signal (label: {y[index]})')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_signal_windows(X, labels=None, class_to_plot=None, num_samples=5, title=\"Signal Windows\"):\n",
    "    \"\"\"\n",
    "    Plot multiple signal windows for visualization\n",
    "\n",
    "    Args:\n",
    "        X: Input data array\n",
    "        labels: Optional labels array\n",
    "        class_to_plot: If provided, only plot windows from this class\n",
    "        num_samples: Number of windows to plot\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Select indices to plot\n",
    "    if labels is not None and class_to_plot is not None:\n",
    "        # Get indices of samples from the specified class\n",
    "        class_indices = np.where(labels == class_to_plot)[0]\n",
    "        # Randomly select from those indices\n",
    "        if len(class_indices) > num_samples:\n",
    "            indices_to_plot = np.random.choice(class_indices, num_samples, replace=False)\n",
    "        else:\n",
    "            indices_to_plot = class_indices\n",
    "    else:\n",
    "        # Randomly select from all samples\n",
    "        indices_to_plot = np.random.choice(len(X), min(num_samples, len(X)), replace=False)\n",
    "\n",
    "    for i, idx in enumerate(indices_to_plot):\n",
    "        sample = X[idx]\n",
    "        # window_size = sample.shape[0]\n",
    "        window_size = sample.shape[0] // 2\n",
    "\n",
    "\n",
    "        # Extract ECG and EDA\n",
    "        ecg = sample[:window_size, 0]\n",
    "        # eda = sample[:window_size, 1]\n",
    "\n",
    "        # Plot ECG\n",
    "        plt.subplot(num_samples, 2, 2*i+1)\n",
    "        plt.plot(ecg, 'r-')\n",
    "        label_text = f\"Window {idx}\"\n",
    "        if labels is not None:\n",
    "            label_text += f\" (Class {labels[idx]})\"\n",
    "        plt.title(f\"ECG {label_text}\")\n",
    "\n",
    "        # Plot EDA\n",
    "        # plt.subplot(num_samples, 2, 2*i+2)\n",
    "        # plt.plot(eda, 'b-')\n",
    "        # plt.title(f\"EDA {label_text}\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.92)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08ab515",
   "metadata": {},
   "source": [
    "## Define Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131f749",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size=1, bidirectional=True):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        # Update the input size of the fully connected layer\n",
    "        direction_factor = 2 if bidirectional else 1\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * direction_factor)\n",
    "        self.fc1 = nn.Linear(hidden_size * direction_factor, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, output_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gru_out, _ = self.gru(x)  # (batch, seq_len, hidden*dir)\n",
    "        gru_out = self.layer_norm(gru_out)\n",
    "        last_hidden = gru_out[:, -1, :]  # get last time step\n",
    "        x = F.relu(self.fc1(last_hidden))\n",
    "        x = self.dropout(x)\n",
    "        out = self.fc2(x)\n",
    "        return out  # raw logits\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "            nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.GRU):\n",
    "            for name, param in m.named_parameters():\n",
    "                if 'weight' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                elif 'bias' in name:\n",
    "                    nn.init.zeros_(param)\n",
    "\n",
    "class CNNStressClassifier(nn.Module):\n",
    "    def __init__(self, input_channels=2, num_classes=2):\n",
    "        super(CNNStressClassifier, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=7, padding=3)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # Výstup: (batch_size, 128, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(128, 64)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, channels=2, sequence_length)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "\n",
    "        x = self.global_pool(x)        # shape: (batch, 128, 1)\n",
    "        x = x.squeeze(-1)              # shape: (batch, 128)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291a463e",
   "metadata": {},
   "source": [
    "## Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74a04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, epochs=10, device='cpu'):\n",
    "    \"\"\"\n",
    "    Train the model\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        train_loader: Training data loader\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to use for training\n",
    "\n",
    "    Returns:\n",
    "        Trained model\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Calculate class balance in training set\n",
    "    all_labels = []\n",
    "    for _, y_batch in train_loader:\n",
    "        all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    unique_labels, label_counts = np.unique(np.array(all_labels), return_counts=True)\n",
    "    print(f\"Training data class distribution: {dict(zip(unique_labels, label_counts))}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
    "        epoch_iter = tqdm(train_loader, desc=f\"Epoch {epoch + 1}\", leave=False)\n",
    "\n",
    "        total_loss = 0\n",
    "        correct, total = 0, 0\n",
    "\n",
    "        for X_batch, y_batch in epoch_iter:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = y_batch.view(-1).float()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch).squeeze()  # Ensures shape [batch_size]\n",
    "\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predicted = (torch.sigmoid(outputs) > 0.6).float()\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "            total += y_batch.size(0)\n",
    "\n",
    "        # Calculate and log training metrics\n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = correct / total * 100\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "\n",
    "        # Check prediction distribution periodically\n",
    "        if epoch % 5 == 0 or epoch == epochs - 1:\n",
    "            check_prediction_distribution(model, train_loader, device)\n",
    "\n",
    "    return model\n",
    "\n",
    "def check_prediction_distribution(model, data_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Check the distribution of predictions\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in data_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "            preds = (torch.sigmoid(outputs) > 0.6).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "\n",
    "    unique_preds, pred_counts = np.unique(np.array(all_preds), return_counts=True)\n",
    "    print(f\"Prediction distribution: {dict(zip(unique_preds, pred_counts))}\")\n",
    "    model.train()\n",
    "\n",
    "def evaluate_model(model, test_loader, threshold=0.6, device='cpu'):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        test_loader: Test data loader\n",
    "        threshold: Classification threshold\n",
    "        device: Device to use for evaluation\n",
    "\n",
    "    Returns:\n",
    "        true_labels: Ground truth labels\n",
    "        predicted_labels: Predicted labels\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_outputs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch).squeeze()\n",
    "\n",
    "            all_outputs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(y_batch.cpu().numpy())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    all_outputs = np.array(all_outputs)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    # Apply threshold to get binary predictions\n",
    "    predicted_labels = (sigmoid(all_outputs) > threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = (predicted_labels == all_labels).mean()\n",
    "    f1 = f1_score(all_labels, predicted_labels)\n",
    "    balanced_acc = balanced_accuracy_score(all_labels, predicted_labels)\n",
    "\n",
    "    print(f\"\\nEvaluation metrics:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "\n",
    "    return all_labels, predicted_labels\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Sigmoid function for numpy arrays\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Low Load\", \"High Load\"])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall/Sensitivity: {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "def save_model(model, path):\n",
    "    \"\"\"\n",
    "    Save model to disk\n",
    "    \"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path, model):\n",
    "    \"\"\"\n",
    "    Load model from disk\n",
    "    \"\"\"\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bdc69",
   "metadata": {},
   "source": [
    "## Google Drive Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to mount Google Drive\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435de28",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09c41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for loading pre-processed data from Drive\n",
    "def download_data():\n",
    "    \"\"\"\n",
    "    Download example data or use existing data\n",
    "    \"\"\"\n",
    "    # Check if data exists\n",
    "    if not os.path.exists(\"drive\"):\n",
    "        os.makedirs(\"drive\", exist_ok=True)\n",
    "        os.makedirs(\"drive/MyDrive/ProjectData/Low_load\", exist_ok=True)\n",
    "        os.makedirs(\"drive/MyDrive/ProjectData/High_load\", exist_ok=True)\n",
    "\n",
    "        print(\"Please upload your ECG/EDA data files to the datasets directory\")\n",
    "        print(\"The structure should be:\")\n",
    "        print(\"- datasets/Low_load/\")\n",
    "        print(\"- datasets/High_load/\")\n",
    "        print(\"With CSV files in the format: subject_block_signal.csv\")\n",
    "        print(\"Example: 1_4_ecg.csv, 1_4_eda.csv\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Check for data\n",
    "    if download_data():\n",
    "        # Load dataset\n",
    "        base_dir = \"drive/MyDrive/ProjectData\"\n",
    "        print(\"Loading data from folders...\")\n",
    "        X, y, sample_weights = load_data_from_folders(base_dir, balance_classes=True)\n",
    "\n",
    "        # Plot some examples\n",
    "        plot_signal_windows(X, labels=y, class_to_plot=0, num_samples=3, title=\"Low Cognitive Load Samples (ECG only)\")\n",
    "        plot_signal_windows(X, labels=y, class_to_plot=1, num_samples=3, title=\"High Cognitive Load Samples (ECG only)\")\n",
    "        # plot_signal_windows(X, labels=y, class_to_plot=0, num_samples=3, title=\"Low Cognitive Load Samples\")\n",
    "        # plot_signal_windows(X, labels=y, class_to_plot=1, num_samples=3, title=\"High Cognitive Load Samples\")\n",
    "\n",
    "\n",
    "        # Define model hyperparameters\n",
    "        input_size = 1  # ECG channel only\n",
    "        # input_size = 2  # ECG and EDA channels\n",
    "        hidden_size = 250\n",
    "        output_size = 1\n",
    "        learning_rate = 1e-3\n",
    "        batch_size = 64\n",
    "\n",
    "        # Create balanced data loaders\n",
    "        train_loader, test_loader = create_data_loaders(X, y, sample_weights, batch_size)\n",
    "\n",
    "        # Initialize model\n",
    "        print(\"Initializing model...\")\n",
    "        # model = GRUModel(input_size, hidden_size, output_size, bidirectional=False).to(device)\n",
    "        model = CNNStressClassifier(input_size, output_size).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "        # Define model path\n",
    "        # model_path = \"cognitive_load_model_ecg_only.pth\"\n",
    "        model_path = \"trained_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model = train_model(model, train_loader, criterion, optimizer, epochs=50, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12cd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e61aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "load_model(model_path, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3168ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "true_labels, predicted_labels = evaluate_model(model, test_loader, device=device)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f860dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_window(model, window):\n",
    "    with torch.no_grad():\n",
    "        x = torch.tensor(window, dtype=torch.float32)\n",
    "        logits = model(x.unsqueeze(0))\n",
    "        pred = torch.argmax(logits).item()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09e3ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_classification(signal, window_size, labels, preds):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(signal, label=\"Fyziologický signál\")\n",
    "\n",
    "    for i, (true, pred) in enumerate(zip(labels, preds)):\n",
    "        start = i * window_size\n",
    "        end = start + window_size\n",
    "\n",
    "        color = 'green' if true == pred else 'red'\n",
    "        plt.axvspan(start, end, color=color, alpha=0.3)\n",
    "\n",
    "    plt.title(\"Vizualizace klasifikace kognitivní zátěže\")\n",
    "    plt.xlabel(\"Čas (vzorky)\")\n",
    "    plt.ylabel(\"Intenzita signálu\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
